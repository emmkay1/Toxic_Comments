{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from time import time\n",
    "from tensorflow.python.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "EMBEDDING_DIM = 50\n",
    "VALIDATION_SPLIT = 0.2\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dir = 'D:/Downloads - data drive/glove.6B'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.%sd.txt' %EMBEDDING_DIM), encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "sentences = train[\"comment_text\"].fillna(\"DUMMY_VALUE\").values\n",
    "possible_labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "targets = train[possible_labels].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(MAX_VOCAB_SIZE, len( word_index) + 1)\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in  word_index.items():\n",
    "    if i < MAX_VOCAB_SIZE:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "  num_words,\n",
    "  EMBEDDING_DIM,\n",
    "  weights=[embedding_matrix],\n",
    "  input_length=MAX_SEQUENCE_LENGTH,\n",
    "  trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "x = embedding_layer(input_)\n",
    "x = Bidirectional(LSTM(15, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "output = Dense(len(possible_labels), activation=\"sigmoid\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_, output)\n",
    "model.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  optimizer=Adam(lr=0.01),\n",
    "  metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = model.fit(\n",
    "  data,\n",
    "  targets,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  epochs=EPOCHS,\n",
    "  validation_split=VALIDATION_SPLIT,\n",
    "  callbacks=[tensorboard]  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('model_tokenizer/pre_trained_toxic_model.h5')\n",
    "\n",
    "# with open('model_tokenizer/tokenizer.pickle', 'wb') as handle:\n",
    "#     pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# accuracies\n",
    "plt.plot(r.history['acc'], label='acc')\n",
    "plt.plot(r.history['val_acc'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "p = model.predict(data)\n",
    "aucs = []\n",
    "for j in range(6):\n",
    "    auc = roc_auc_score(targets[:,j], p[:,j])\n",
    "    aucs.append(auc)\n",
    "print(np.mean(aucs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/test.csv\")\n",
    "test_sentences = test[\"comment_text\"]\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "x_test = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model.predict(x_test)\n",
    "predicted_file = pd.read_csv(\"data/sample_submission.csv\")\n",
    "predicted_file[possible_labels] = y_test\n",
    "predicted_file.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('submission.csv')\n",
    "\n",
    "COLUMNS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "df_train['none'] = (df_train[COLUMNS].max(axis=1) == 0).astype(int)\n",
    "COLUMNS.append('none')\n",
    "CATEGORIES = COLUMNS.copy()\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_train.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distribution = df_train[COLUMNS].sum()\\\n",
    "                                    .to_frame()\\\n",
    "                                    .rename(columns={0: 'count'})\\\n",
    "                                    .sort_values('count')\n",
    "\n",
    "df_distribution.drop('none').plot.pie(y='count', \n",
    "                                      title='Label distribution over comments (without \"none\" category)',\n",
    "                                      figsize=(5, 5))\\\n",
    "                                    .legend(loc='center left', bbox_to_anchor=(1.3, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_list = [\"brownish green\", \"pine green\", \"ugly purple\",\n",
    "               \"blood\", \"deep blue\", \"brown\", \"azure\"]\n",
    "\n",
    "palette= sns.xkcd_palette(colors_list)\n",
    "\n",
    "x=df_train.iloc[:,1:].sum().round()\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "ax= sns.barplot(x.index, x.values,palette=palette)\n",
    "plt.title(\"Class\")\n",
    "plt.ylabel('Occurrences', fontsize=12)\n",
    "plt.xlabel('Type ')\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 10, label, \n",
    "            ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting word clouds for train data\n",
    "\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "\n",
    "COLUMNS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "df_train['none'] = (df_train[COLUMNS].max(axis=1) == 0).astype(int)\n",
    "COLUMNS.append('none')\n",
    "CATEGORIES = COLUMNS.copy()\n",
    "\n",
    "word_counter = {}\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub('[{}]'.format(string.punctuation), ' ', text.lower())\n",
    "    return ' '.join([word for word in text.split() if word not in (stop)])\n",
    "\n",
    "for categ in CATEGORIES:\n",
    "    d = Counter()\n",
    "    df_train[df_train[categ] == 1]['comment_text'].apply(lambda t: d.update(clean_text(t).split()))\n",
    "    word_counter[categ] = pd.DataFrame.from_dict(d, orient='index')\\\n",
    "                                        .rename(columns={0: 'count'})\\\n",
    "                                        .sort_values('count', ascending=False)\n",
    "for w in word_counter:\n",
    "    wc = word_counter[w]\n",
    "\n",
    "    wordcloud = WordCloud(\n",
    "          background_color='black',\n",
    "          max_words=200,\n",
    "          max_font_size=60, \n",
    "          random_state=4561\n",
    "         ).generate_from_frequencies(wc.to_dict()['count'])\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 13))\n",
    "    plt.title(w)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
